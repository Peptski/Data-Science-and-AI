{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sTsDfIVKsmL"
   },
   "source": [
    "# DAT405 Introduction to Data Science and AI \n",
    "## Assignment 4: Spam classification using Naïve Bayes \n",
    "\n",
    "The exercise takes place in a notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \n",
    "Hints:\n",
    "You can execute certain linux shell commands by prefixing the command with `!`. You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results the second you can use writing code snippets that execute the tasks required.  \n",
    "\n",
    "In this assignment you will implement a Naïve Bayes classifier in Python that will classify emails into spam and non-spam (“ham”) classes.  Your program should be able to train on a given set of spam and “ham” datasets. \n",
    "You will work with the datasets available at https://spamassassin.apache.org/old/publiccorpus/. There are three types of files in this location: \n",
    "-\teasy-ham: non-spam messages typically quite easy to differentiate from spam messages. \n",
    "-\thard-ham: non-spam messages more difficult to differentiate \n",
    "-\tspam: spam messages \n",
    "\n",
    "**Execute the cell below to download and extract the data into the environment of the notebook -- it will take a few seconds.** If you chose to use Jupyter notebooks you will have to run the commands in the cell below on your local computer, with Windows you can use 7zip (https://www.7-zip.org/download.html) to decompress the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributors:\n",
    "### Lukas Andersson - 35 Hours\n",
    "### Ramapriya Navalpakkam -  30 hours\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Wa37fBwRF-xe"
   },
   "outputs": [],
   "source": [
    "#Download and extract data\n",
    "# !wget https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
    "# !wget https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
    "# !wget https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
    "# !tar -xjf 20021010_easy_ham.tar.bz2\n",
    "# !tar -xjf 20021010_hard_ham.tar.bz2\n",
    "# !tar -xjf 20021010_spam.tar.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdH1XTepLjZ3"
   },
   "source": [
    "*The* data is now in the three folders `easy_ham`, `hard_ham`, and `spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A53Gw00fBLG2"
   },
   "outputs": [],
   "source": [
    "# !ls -lah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGlWPVnSNzT7"
   },
   "source": [
    "###1. Preprocessing: \n",
    "1.\tNote that the email files contain a lot of extra information, besides the actual message. Ignore that and run on the entire text. \n",
    "2.\tWe don’t want to train and test on the same data. Split the spam and the ham datasets in a training set and a test set. (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`) **0.5p**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "J2sllUWXKblD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "dirs_easy = os.listdir('../data/easy_ham')\n",
    "dirs_hard = os.listdir('../data/hard_ham')\n",
    "dirs_spam = os.listdir('../data/spam')\n",
    "\n",
    "easyham = pd.DataFrame([open(\"../data/easy_ham/\" + file, \"r\", encoding=\"iso-8859-1\").read() for file in dirs_easy]) \n",
    "hardham = pd.DataFrame([open(\"../data/hard_ham/\" + file, \"r\", encoding=\"iso-8859-1\").read() for file in dirs_hard]) \n",
    "spam = pd.DataFrame([open(\"../data/spam/\" + file, \"r\", encoding=\"iso-8859-1\").read() for file in dirs_spam])\n",
    "\n",
    "easyham['label'] = 0\n",
    "easyham.columns = ['message', 'label']\n",
    "hardham['label'] = 0\n",
    "hardham.columns = ['message', 'label']\n",
    "spam['label'] = 1\n",
    "spam.columns = ['message', 'label']\n",
    "\n",
    "all_frames = [easyham, hardham, spam]\n",
    "easy_frames = [easyham, spam]\n",
    "\n",
    "all_data = pd.concat(all_frames)\n",
    "easy_data = pd.concat(easy_frames)\n",
    "\n",
    "X = CountVectorizer().fit_transform(all_data['message'])    \n",
    "y = all_data['label']\n",
    "\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(X, y, test_size=0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnbrbI0_OKCF"
   },
   "source": [
    "###2. Write a Python program that: \n",
    "1.\tUses four datasets (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`) \n",
    "2.\tTrains a Naïve Bayes classifier (e.g. Sklearn) on `hamtrain` and `spamtrain`, that classifies the test sets and reports True Positive and True Negative rates on the `hamtest` and `spamtest` datasets. You can use `CountVectorizer` to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifier in SKlearn ([Documentation here](https://scikit-learn.org/stable/modules/naive_bayes.html)). Test two of these classifiers that are well suited for this problem\n",
    "    - Multinomial Naive Bayes  \n",
    "    - Bernoulli Naive Bayes. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MJERHSCcGNaW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: True Positive rate = 0.8896551724137931 and True Negative rate = 0.9911894273127754\n",
      "Bernoulli   Naive Bayes: True Positive rate = 0.21379310344827587 and True Negative rate = 0.9823788546255506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(hamtrain, spamtrain)\n",
    "mnb_tn, mnb_fp, mnb_fn, mnb_tp = confusion_matrix(spamtest, mnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "bnb_classifier = BernoulliNB()\n",
    "bnb_classifier.fit(hamtrain, spamtrain)\n",
    "bnb_tn, bnb_fp, bnb_fn, bnb_tp = confusion_matrix(spamtest, bnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "print(\"Multinomial Naive Bayes: True Positive rate = \" + str(mnb_tp) + \" and True Negative rate = \" + str(mnb_tn))\n",
    "print(\"Bernoulli   Naive Bayes: True Positive rate = \" + str(bnb_tp) + \" and True Negative rate = \" + str(bnb_tn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slb-ipX4TMFb"
   },
   "source": [
    "a) Explain how the classifiers differ. What different interpretations do they have? **1p** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI1bPDCvQxen"
   },
   "source": [
    "Multinomial counts how many times a specific feature occurs while Bernoulli simply models if the feature occurs or not, but does not count.\n",
    "\n",
    "Above we printed out the True Positive and True Negative for both methods, in our tests Multinomial performed way better, especially on finding real ham (True Positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDFS3uFFUcS7"
   },
   "source": [
    "### 3.Run your program on \n",
    "-\tSpam versus easy-ham \n",
    "-\tSpam versus (hard-ham + easy-ham). \n",
    "-   Discuss your results **2.5p** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gool_zb8Qzzy",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: True Positive rate = 0.9310344827586207 and True Negative rate = 0.9984544049459042 for Easy-ham ONLY\n",
      "Bernoulli   Naive Bayes: True Positive rate = 0.46551724137931033 and True Negative rate = 0.9969088098918083 for Easy-ham ONLY\n",
      "Multinomial Naive Bayes: True Positive rate = 0.8896551724137931 and True Negative rate = 0.9911894273127754 for Easy- and Hard-ham\n",
      "Bernoulli   Naive Bayes: True Positive rate = 0.21379310344827587 and True Negative rate = 0.9823788546255506 for Easy- and Hard-ham\n"
     ]
    }
   ],
   "source": [
    "#Easy-ham\n",
    "\n",
    "X = CountVectorizer().fit_transform(easy_data['message'])    \n",
    "y = easy_data['label']\n",
    "\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(X, y, test_size=0.25, random_state = 0)\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(hamtrain, spamtrain)\n",
    "mnb_tn, mnb_fp, mnb_fn, mnb_tp = confusion_matrix(spamtest, mnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "bnb_classifier = BernoulliNB()\n",
    "bnb_classifier.fit(hamtrain, spamtrain)\n",
    "bnb_tn, bnb_fp, bnb_fn, bnb_tp = confusion_matrix(spamtest, bnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "print(\"Multinomial Naive Bayes: True Positive rate = \" + str(mnb_tp) + \" and True Negative rate = \" + str(mnb_tn) + \" for Easy-ham ONLY\")\n",
    "print(\"Bernoulli   Naive Bayes: True Positive rate = \" + str(bnb_tp) + \" and True Negative rate = \" + str(bnb_tn) + \" for Easy-ham ONLY\")\n",
    "\n",
    "\n",
    "#Hard-ham + Easy-ham\n",
    "\n",
    "X = CountVectorizer().fit_transform(all_data['message'])    \n",
    "y = all_data['label']\n",
    "\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(X, y, test_size=0.25, random_state = 0)\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(hamtrain, spamtrain)\n",
    "mnb_tn, mnb_fp, mnb_fn, mnb_tp = confusion_matrix(spamtest, mnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "bnb_classifier = BernoulliNB()\n",
    "bnb_classifier.fit(hamtrain, spamtrain)\n",
    "bnb_tn, bnb_fp, bnb_fn, bnb_tp = confusion_matrix(spamtest, bnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "print(\"Multinomial Naive Bayes: True Positive rate = \" + str(mnb_tp) + \" and True Negative rate = \" + str(mnb_tn) + \" for Easy- and Hard-ham\")\n",
    "print(\"Bernoulli   Naive Bayes: True Positive rate = \" + str(bnb_tp) + \" and True Negative rate = \" + str(bnb_tn) + \" for Easy- and Hard-ham\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They both performed better on the easy-ham only version, even though the difference was quite slight for Multinomial while it was quite large for Bernoulli. This however is probably just because it is easier data. The hard ham is much more similar to spam and therefore harder to distinguish as ham. This also means that the easy only got worse training than the mixed one, since it did not train on hard ham. It would probably be way better performance if it only trained on hard-ham since those are the features we want to distinguish to make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkfQWBB4UhYd"
   },
   "source": [
    "# 4.\tTo avoid classification based on common and uninformative words it is common to filter these out. \n",
    "\n",
    "**a.** Argue why this may be useful. Try finding the words that are too common/uncommon in the dataset. **1p** \n",
    "\n",
    "\n",
    "\n",
    "**b.** Use the parameters in Sklearn’s `CountVectorizer` to filter out these words. Update the program from point 3 and run it on your data and report and discuss your results. You have two options to do this in Sklearn: either using the words found in part (a) or letting Sklearn do it for you. Argue for your decision-making. **1p** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDvErN_1TMFc"
   },
   "source": [
    "- Words that are too common as we see are mostly words like 'a', 'is', 'and'. These do not give us any information about the text and would certainly not aid in determining if the words are ham or spam words. Therefore removing these words would optimise the performance of the model as it would have fewer and more meaningful words to train on. Below we remove special characters, numbers and escape sequences before we fit the model to the counter. This ensures that the model skips learning from these unimportant characters and symbols when trying to classify ham and spam.\n",
    "\n",
    "- We chose to use Sklearn's algorithm to filter out the most common words from the dataset.  The `max_df` parameter allows us to set a threashold above with the words having their frequency is ignored. We have set this value to 60% so the algorithm gives a 10% extra slack for words to have a slightly higher frequency the the others. This is a very convenient method to filter out the words compared to loop through each word found in the previous section to eliminiate it in the dataset. This can take a veyr long time. Both of these methods can however be applied only to the dataset after a preproccessing steps where we eliminate symbols, numbers and escape sequences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qt7ELzEqUfas"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most uncommon words : [('Remove', 1), ('subject', 1), ('cn', 1), ('btamail', 1), ('bm', 1), ('mailto', 1), ('email', 1), ('blank', 1), ('send', 1), ('link', 1), ('click', 1), ('list', 1), ('address', 1), ('remove', 1), ('return', 1), ('convenient', 1), ('phone', 1), ('name', 1), ('Leave', 1), ('automated', 1), ('enjoy', 1), ('Beyond', 1), ('Priority', 1), ('international', 1), ('domestic', 1), ('Postal', 1), ('US', 1), ('next', 1), ('shipped', 1), ('orders', 1)]\n",
      "\n",
      "The most common words are: [('of', 84), ('is', 63), ('and', 62), ('the', 59), ('to', 56), ('a', 50), ('for', 48), ('or', 45), ('as', 27), ('One', 27), ('Shangrila', 26), ('not', 24), ('tm', 23), ('Zowie', 23), ('Wowie', 23), ('that', 23), ('in', 22), ('be', 20), ('it', 19), ('botanical', 19), ('at', 19), ('this', 19), ('oz', 19), ('any', 18), ('product', 17), ('with', 16), ('non', 16), ('are', 15), ('has', 15), ('Kathmandu', 15)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "def count_words():\n",
    "\n",
    "    for message in all_data['message']:\n",
    "        \n",
    "        message = message.translate ({ord(c): \" \" for c in \"!@#$%^&*()[]{};:,./<>?\\|`~-=_+=\"})\n",
    "        message = re.sub(r'[0-9]+', ' ', message) \n",
    "        message = message.replace('\\n', ' ')\n",
    "        message = message.replace('\\t', ' ')\n",
    "        \n",
    "        \n",
    "        # split the mails into words\n",
    "        mails_splitted = message.split()\n",
    "        word_counter = Counter()\n",
    "        # count how many times a word occurs in all emails\n",
    "        word_counter = word_counter + Counter(mails_splitted)\n",
    "        \n",
    "    return word_counter\n",
    "\n",
    "\n",
    "\n",
    "word_counter = count_words()\n",
    "\n",
    "#number of words\n",
    "num_words = 30\n",
    "\n",
    "#the least common words\n",
    "most_uncommon_words = word_counter.most_common()[:-num_words-1:-1]\n",
    "print(\"The most uncommon words :\", most_uncommon_words)\n",
    "\n",
    "#the top common words\n",
    "most_common_words = word_counter.most_common(num_words)\n",
    "print('\\nThe most common words are:', most_common_words)\n",
    "\n",
    "#popular_words = sorted(word_counter, key = word_counter.get, reverse = False)\n",
    "\n",
    "#print(popular_words[:-num_words-1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: True Positive rate = 0.9396551724137931 and True Negative rate = 0.9984544049459042 for Easy-ham ONLY\n",
      "Bernoulli   Naive Bayes: True Positive rate = 0.3879310344827586 and True Negative rate = 0.9969088098918083 for Easy-ham ONLY\n",
      "Multinomial Naive Bayes: True Positive rate = 0.9655172413793104 and True Negative rate = 0.9882525697503671 for Easy- and Hard-ham\n",
      "Bernoulli   Naive Bayes: True Positive rate = 0.2 and True Negative rate = 0.9809104258443465 for Easy- and Hard-ham\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "#nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Easy-ham\n",
    "\n",
    "X = CountVectorizer(max_df=0.6, stop_words=stopwords.words('english')).fit_transform(easy_data['message'])\n",
    "\n",
    "y = easy_data['label']\n",
    "\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(X, y, test_size=0.25, random_state = 0)\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(hamtrain, spamtrain)\n",
    "mnb_tn, mnb_fp, mnb_fn, mnb_tp = confusion_matrix(spamtest, mnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "bnb_classifier = BernoulliNB()\n",
    "bnb_classifier.fit(hamtrain, spamtrain)\n",
    "bnb_tn, bnb_fp, bnb_fn, bnb_tp = confusion_matrix(spamtest, bnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "print(\"Multinomial Naive Bayes: True Positive rate = \" + str(mnb_tp) + \" and True Negative rate = \" + str(mnb_tn) + \" for Easy-ham ONLY\")\n",
    "print(\"Bernoulli   Naive Bayes: True Positive rate = \" + str(bnb_tp) + \" and True Negative rate = \" + str(bnb_tn) + \" for Easy-ham ONLY\")\n",
    "\n",
    "\n",
    "#Hard-ham + Easy-ham\n",
    "\n",
    "X = CountVectorizer(max_df=0.6,stop_words=stopwords.words('english')).fit_transform(all_data['message'])\n",
    "y = all_data['label']\n",
    "\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(X, y, test_size=0.25, random_state = 0)\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(hamtrain, spamtrain)\n",
    "mnb_tn, mnb_fp, mnb_fn, mnb_tp = confusion_matrix(spamtest, mnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "bnb_classifier = BernoulliNB()\n",
    "bnb_classifier.fit(hamtrain, spamtrain)\n",
    "bnb_tn, bnb_fp, bnb_fn, bnb_tp = confusion_matrix(spamtest, bnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "print(\"Multinomial Naive Bayes: True Positive rate = \" + str(mnb_tp) + \" and True Negative rate = \" + str(mnb_tn) + \" for Easy- and Hard-ham\")\n",
    "print(\"Bernoulli   Naive Bayes: True Positive rate = \" + str(bnb_tp) + \" and True Negative rate = \" + str(bnb_tn) + \" for Easy- and Hard-ham\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDvErN_1TMFc"
   },
   "source": [
    "- In the results we see that there is a significant improvement the identification of true positives in the Multinomial bayes classification of easy-ham and hard-ham vs spam than that of easy-ham vs spam. On the other hand, the there is a drop in the prediction of true positives by the Bernoulli's classification. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcyVfOZFU4F_"
   },
   "source": [
    "### 5. Eeking out further performance\n",
    "**a.**  Use a lemmatizer to normalize the text (for example from the `nltk` library). For one implementation look at the documentation ([here](https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes)). Run your program again and answer the following questions: \n",
    "  - Why can lemmatization help?\n",
    "  -\tDoes the result improve from 3 and 4? Discuss. **1.5p** \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ6vCyUlTMFc"
   },
   "source": [
    "- Lemmatisation transforms words to their orginial or base forms. For example, serialise, serialize, serialisation are different forms of the same word serial. A lemmatization on any of these words we can obtain the word serial. Lemmatization of words makes analysis on the dataset simpler by reducing the number of words to be analysed. In our case, the model can now easily learn to differentiate between ham or spam words and avoids unnecessary time learning different forms of the same word. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: True Positive rate = 0.9827586206896551 and True Negative rate = 0.9984544049459042 for Easy-ham ONLY\n",
      "Bernoulli   Naive Bayes: True Positive rate = 0.5 and True Negative rate = 0.9969088098918083 for Easy-ham ONLY\n",
      "Multinomial Naive Bayes: True Positive rate = 0.9793103448275862 and True Negative rate = 0.9853157121879589 for Easy- and Hard-ham\n",
      "Bernoulli   Naive Bayes: True Positive rate = 0.2896551724137931 and True Negative rate = 0.973568281938326 for Easy- and Hard-ham\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize \n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "\n",
    "#Easy-ham\n",
    "\n",
    "X = CountVectorizer(max_df=0.6, tokenizer=LemmaTokenizer()\n",
    ").fit_transform(easy_data['message'])\n",
    "\n",
    "y = easy_data['label']\n",
    "\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(X, y, test_size=0.25, random_state = 0)\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(hamtrain, spamtrain)\n",
    "mnb_tn, mnb_fp, mnb_fn, mnb_tp = confusion_matrix(spamtest, mnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "bnb_classifier = BernoulliNB()\n",
    "bnb_classifier.fit(hamtrain, spamtrain)\n",
    "bnb_tn, bnb_fp, bnb_fn, bnb_tp = confusion_matrix(spamtest, bnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "print(\"Multinomial Naive Bayes: True Positive rate = \" + str(mnb_tp) + \" and True Negative rate = \" + str(mnb_tn) + \" for Easy-ham ONLY\")\n",
    "print(\"Bernoulli   Naive Bayes: True Positive rate = \" + str(bnb_tp) + \" and True Negative rate = \" + str(bnb_tn) + \" for Easy-ham ONLY\")\n",
    "\n",
    "\n",
    "#Hard-ham + Easy-ham\n",
    "\n",
    "X = CountVectorizer(max_df=0.7,tokenizer=LemmaTokenizer()).fit_transform(all_data['message'])\n",
    "y = all_data['label']\n",
    "\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(X, y, test_size=0.25, random_state = 0)\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(hamtrain, spamtrain)\n",
    "mnb_tn, mnb_fp, mnb_fn, mnb_tp = confusion_matrix(spamtest, mnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "bnb_classifier = BernoulliNB()\n",
    "bnb_classifier.fit(hamtrain, spamtrain)\n",
    "bnb_tn, bnb_fp, bnb_fn, bnb_tp = confusion_matrix(spamtest, bnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "print(\"Multinomial Naive Bayes: True Positive rate = \" + str(mnb_tp) + \" and True Negative rate = \" + str(mnb_tn) + \" for Easy- and Hard-ham\")\n",
    "print(\"Bernoulli   Naive Bayes: True Positive rate = \" + str(bnb_tp) + \" and True Negative rate = \" + str(bnb_tn) + \" for Easy- and Hard-ham\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDvErN_1TMFc"
   },
   "source": [
    "We can see that adding lemmatisation to the Vectorization has made slight improvements in the Mulitnomial Naive Bayes classificaition and a reduction in the accuracy for the Bernoulli's classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDvErN_1TMFc"
   },
   "source": [
    "**b.** The split of the data set into a training set and a test set can lead to very skewed results. Why is this, and do you have suggestions on remedies? \n",
    " What do you expect would happen if your training set were mostly spam messages while your test set were mostly ham messages?  **1p** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6cjRxQyTMFd"
   },
   "source": [
    "This is because of the exact thing the later part of the question is suggesting, it could be very bad balanced and have big overrepresentations of either class in either test or training sets, which would impact the result a lot. If we trained it on mostly spam messages but had mostly ham in the test set it would probably perform very well at detecting spam, True Negative, but would perform worse on detecting ham, True Positive. It would also probably have quite a lot of False Negative, predicting that it is spam when it in fact is ham.\n",
    "\n",
    "You could always preprocess the data in a way that you handle spam and ham separately and split them separately to then concatenate them into a set after both are individually split. This would ensure that the balance always is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_nyGug9U4f3"
   },
   "source": [
    "**c.** Re-estimate your classifier using `fit_prior` parameter set to `false`, and answer the following questions:\n",
    "  - What does this parameter mean?\n",
    "  - How does this alter the predictions? Discuss why or why not. **0.5p** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes: True Positive rate = 0.9827586206896551 and True Negative rate = 0.9984544049459042 for Easy-ham ONLY\n",
      "Bernoulli   Naive Bayes: True Positive rate = 0.5 and True Negative rate = 0.9969088098918083 for Easy-ham ONLY\n",
      "Multinomial Naive Bayes: True Positive rate = 0.9793103448275862 and True Negative rate = 0.9853157121879589 for Easy- and Hard-ham\n",
      "Bernoulli   Naive Bayes: True Positive rate = 0.2896551724137931 and True Negative rate = 0.973568281938326 for Easy- and Hard-ham\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize \n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "\n",
    "#Easy-ham\n",
    "\n",
    "X = CountVectorizer(max_df=0.6, tokenizer=LemmaTokenizer()\n",
    ").fit_transform(easy_data['message'])\n",
    "\n",
    "y = easy_data['label']\n",
    "\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(X, y, test_size=0.25, random_state = 0)\n",
    "\n",
    "mnb_classifier = MultinomialNB(fit_prior=False)\n",
    "mnb_classifier.fit(hamtrain, spamtrain)\n",
    "mnb_tn, mnb_fp, mnb_fn, mnb_tp = confusion_matrix(spamtest, mnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "bnb_classifier = BernoulliNB(fit_prior=False)\n",
    "bnb_classifier.fit(hamtrain, spamtrain)\n",
    "bnb_tn, bnb_fp, bnb_fn, bnb_tp = confusion_matrix(spamtest, bnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "print(\"Multinomial Naive Bayes: True Positive rate = \" + str(mnb_tp) + \" and True Negative rate = \" + str(mnb_tn) + \" for Easy-ham ONLY\")\n",
    "print(\"Bernoulli   Naive Bayes: True Positive rate = \" + str(bnb_tp) + \" and True Negative rate = \" + str(bnb_tn) + \" for Easy-ham ONLY\")\n",
    "\n",
    "\n",
    "#Hard-ham + Easy-ham\n",
    "\n",
    "X = CountVectorizer(max_df=0.7,tokenizer=LemmaTokenizer()).fit_transform(all_data['message'])\n",
    "y = all_data['label']\n",
    "\n",
    "hamtrain, hamtest, spamtrain, spamtest = train_test_split(X, y, test_size=0.25, random_state = 0)\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "mnb_classifier.fit(hamtrain, spamtrain)\n",
    "mnb_tn, mnb_fp, mnb_fn, mnb_tp = confusion_matrix(spamtest, mnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "bnb_classifier = BernoulliNB()\n",
    "bnb_classifier.fit(hamtrain, spamtrain)\n",
    "bnb_tn, bnb_fp, bnb_fn, bnb_tp = confusion_matrix(spamtest, bnb_classifier.predict(hamtest), normalize='true').ravel()\n",
    "\n",
    "print(\"Multinomial Naive Bayes: True Positive rate = \" + str(mnb_tp) + \" and True Negative rate = \" + str(mnb_tn) + \" for Easy- and Hard-ham\")\n",
    "print(\"Bernoulli   Naive Bayes: True Positive rate = \" + str(bnb_tp) + \" and True Negative rate = \" + str(bnb_tn) + \" for Easy- and Hard-ham\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txL8eLSGTMFd"
   },
   "source": [
    "This parameter tells the model to weather to learn from its priors or use a uniform prior. Prior is nothing but the predictions that can be made by just looking at the data without any evidence at hand. Here we can see that there is no difference between setting the fit_prior to false or true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBbl22HfTMFd"
   },
   "source": [
    "**d.** The python model includes smoothing (`alpha` parameter ), explain why this can be important. \n",
    "  - What would happen if in the training data set the word 'money' only appears in spam examples? What would the model predict about a message containing the word 'money'? Does the prediction depend on the rest of the message and is that reasonable? Explain your reasoning  **1p** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtGemb2HTMFe"
   },
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ND6FKoexVAhW"
   },
   "source": [
    "### What to report and how to hand in.\n",
    "\n",
    "- You will need to clearly report all results in the notebook in a clear and appropriate way, either using plots or code output (f.x. \"print statements\"). \n",
    "- The notebook must be reproducible, that means, we must be able to use the `Run all` function from the `Runtime` menu and reproduce all your results. **Please check this before handing in.** \n",
    "- Save the notebook and share a link to the notebook (Press share in upper left corner, and use `Get link` option. **Please make sure to allow all with the link to open and edit.**\n",
    "- Edits made after submission deadline will be ignored, graders will recover the last saved version before deadline from the revisions history.\n",
    "- **Please make sure all cells are executed and all the output is clearly readable/visible to anybody opening the notebook.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment4 (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
